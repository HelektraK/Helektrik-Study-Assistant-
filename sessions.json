{
  "active_class": "cls_1764299612",
  "uploads": [],
  "recordings": [],
  "b696732e-0fee-4da3-bd19-8d593c794e47": {
    "created_at": "2025-11-28T17:11:26.583165",
    "files": [
      {
        "name": "383Final.pdf",
        "type": "document",
        "text": "\n\n\n\n\n\n\n\n\n\n\n\n",
        "added_at": "2025-11-28T17:11:26.593976"
      }
    ]
  },
  "2374118e-773f-46bb-82d0-a36a05377ac9": {
    "created_at": "2025-11-28T23:21:33.239197",
    "files": [
      {
        "name": "383Final.pdf",
        "type": "document",
        "text": "\n\n\n\n\n\n\n\n\n\n\n\n",
        "added_at": "2025-11-28T23:21:33.254253"
      }
    ]
  },
  "c1ad976b-8e00-4168-854f-a15a5cd90e1b": {
    "created_at": "2025-11-28T23:31:46.558654",
    "files": [
      {
        "name": "COMPSCI_514_HW_4.pdf",
        "type": "document",
        "text": "COMPSCI 514 HW 4\nHelektra Katsoulakis\nNovember 2025\nCore Competency Problems\n1. Rank One Matrices and Matrix Completion (10 points)\n1. (2 points) Suppose A \u2208Rn\u00d7d is a rank-1 matrix, i.e., all rows are multiples of some row\nvector y\u22a4. Without loss of generality, you may assume that y is a unit vector. Prove, without\nappealing to the existence of the SVD, that A can be written as \u03b1xy\u22a4for some unit vector\nx \u2208Rn. Specifically, express \u03b1 and each entry of x in terms of entries of A and y.\n2. (2 points) If A = \u03b1xy\u22a4, find the eigenvector of A\u22a4A with the largest eigenvalue and derive an\nexpression for this largest eigenvalue. To get full marks you need to fully explain your work.\n3. (2 points) Let B \u2208Rn\u00d7d be a partially observed matrix, i.e., we know the values of some of\nthe entries but not others. We say rows i, j \u2208[n] of B are directly comparable if there exists\nk such that Bi,k and Bj,k are both known. Prove that the unobserved entries of B can be\ndeduced from the observed entries of B given the following four assumptions:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 For all i \u2208{1, 2, . . . , n \u22121}, the ith and (i + 1)st rows are directly comparable.\n4. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cd)\nn\nfor some sufficiently large constant c then with probability at least 9/10, there exists an\nobserved entry in every column.\n5. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cn)\n\u221a\nd\nfor some sufficiently large constant c then with probability at least 9/10, for all i \u2208{1, 2, . . . , n\u2212\n1}, the ith and (i + 1)st rows are directly comparable. Note: It is also possible to prove that\np \u2265\nr\nlog(cn)\nd\nsuffices. This is only stronger than the bound we are asking for so will receive full credit.\n1\n\nSolution:\n1. Since every row of A is a scalar multiple of y\u22a4, for each i there exists a number ci such that\nAi,: = ci y\u22a4.\nTaking the inner product of the i-th row with y gives\nAi,:y = (ciy\u22a4)y = ci (y\u22a4y) = ci,\nbecause y is a unit vector. Therefore each coefficient is\nci = Ai,:y =\nd\nX\nj=1\nAijyj,\nand the vector z = (c1, . . . , cn)\u22a4is exactly Ay. This shows that\nA = zy\u22a4= (Ay) y\u22a4.\nSince A is rank-1 and nonzero, the vector Ay is not the zero vector. Let\n\u03b1 = \u2225Ay\u22252 =\nv\nu\nu\nu\nt\nn\nX\ni=1\n\uf8eb\n\uf8ed\nd\nX\nj=1\nAijyj\n\uf8f6\n\uf8f8\n2\n,\nx =\nAy\n\u2225Ay\u22252\n.\nThen x is a unit vector by construction, and substituting into the expression for A gives\nA = (Ay) y\u22a4= \u03b1xy\u22a4.\nFinally, each entry of x is\nxi = (Ay)i\n\u2225Ay\u22252\n=\nd\nX\nj=1\nAijyj\nv\nu\nu\nt\nn\nX\nk=1\n d\nX\n\u2113=1\nAk\u2113y\u2113\n!2 .\nWith these definitions of \u03b1 and x, we have shown that\nA = \u03b1xy\u22a4\nfor some unit vector x \u2208Rn.\n2. First compute\nA\u22a4= (\u03b1xy\u22a4)\u22a4= \u03b1yx\u22a4.\nThen\nA\u22a4A = (\u03b1yx\u22a4)(\u03b1xy\u22a4) = \u03b12 y(x\u22a4x) y\u22a4.\n2\n\nSince x is a unit vector, x\u22a4x = \u2225x\u22252\n2 = 1, so this simplifies to\nA\u22a4A = \u03b12 yy\u22a4.\nThis is a rank-1 matrix in Rd\u00d7d with range equal to the span of y.\nNow check what A\u22a4A does to the vector y:\nA\u22a4A y = \u03b12 yy\u22a4y = \u03b12 (y\u22a4y) y = \u03b12 \u00b7 1 \u00b7 y = \u03b12y,\nusing again that y\u22a4y = 1. This shows that y is an eigenvector of A\u22a4A with eigenvalue \u03b12.\nNext, take any vector z \u2208Rd that is orthogonal to y, so y\u22a4z = 0. Then\nA\u22a4A z = \u03b12 yy\u22a4z = \u03b12 y \u00b7 0 = 0.\nThus every vector orthogonal to y is an eigenvector with eigenvalue 0. Therefore the eigen-\nvalues of A\u22a4A are\n\u03bbmax = \u03b12,\nand all remaining eigenvalues are 0.\nThe eigenvector corresponding to the largest eigenvalue \u03b12 is any nonzero multiple of y, and\nif we want a unit eigenvector, we can simply take y itself.\n3. Since B has rank 1, it can be written as\nB = uv\u22a4,\nso each entry factors as\nBij = uivj.\nThis means every row is a scalar multiple of the same vector v, and every column is a scalar\nmultiple of the same vector u.\nFirst, notice that under the assumptions, every row and every column must contain at least\none observed (and nonzero) entry. Because all observed entries satisfy Bij = uivj \u0338= 0, we\nimmediately get that ui \u0338= 0 and vj \u0338= 0 whenever Bij is observed. Since each row participates\nin at least one directly comparable pair, every row has at least one known, nonzero entry, and\ntherefore every ui is nonzero. Similarly, since every column has at least one observed entry,\nevery vj is nonzero as well.\nNow consider two directly comparable rows i and j. By definition, there exists a column k\nsuch that both Bik and Bjk are observed. Using the rank-1 structure,\nBik = uivk,\nBjk = ujvk.\nSince vk \u0338= 0, we divide and get\nBik\nBjk\n= uivk\nujvk\n= ui\nuj\n.\nThus one shared observed entry gives us the ratio ui/uj.\n3\n\nBecause every adjacent pair (i, i + 1) is directly comparable, we can recover all ratios\nu2\nu1\n,\nu3\nu2\n,\n. . . ,\nun\nun\u22121\n.\nMultiplying these together lets us express each ui in terms of u1:\nui\nu1\n=\ni\u22121\nY\nt=1\nut+1\nut\n.\nChoosing any nonzero value for u1 determines every ui uniquely up to an overall scale factor.\nNext, since every column j has at least one observed entry, pick any row i such that Bij is\nknown. Using Bij = uivj and the fact that ui is now known, we can solve for\nvj = Bij\nui\n.\nThis determines every vj.\nFinally, once u and v are known, all entries of B including are determined by\nBij = uivj.\nAfter solving for all of the ui and vj, every entry of B is shown to be the product uivj. Any\ndifferent choice for a missing entry would change this product and would no longer match the\nobserved values, so the missing entries are completely fixed.\n4. Fix a particular column j \u2208{1, . . . , d}.\nThere are n entries in this column, and each is\nobserved independently with probability p. The event that column j has no observed entries\nmeans that all n entries in that column are unobserved, which has probability\nP(column j has no observed entries) = (1 \u2212p)n.\nWe now bound this probability using the inequality 1\u2212p \u2264e\u2212p valid for all real p. This gives\n(1 \u2212p)n \u2264e\u2212pn.\nIf p \u2265log(cd)\nn\n, then\npn \u2265log(cd),\nso\n(1 \u2212p)n \u2264e\u2212pn \u2264e\u2212log(cd) = 1\ncd.\nThus for each fixed column j,\nP(column j has no observed entries) \u22641\ncd.\nWe now apply a union bound over all d columns. The event that at least one column has no\nobserved entries is the union of the events that column j has no observed entries, and so\nP(\u2203a column with no observed entries) \u2264\nd\nX\nj=1\nP(column j has no observed entries) \u2264d\u00b7 1\ncd = 1\nc.\n4\n\nTherefore\nP(every column has at least one observed entry) = 1\u2212P(\u2203a column with no observed entries) \u22651\u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22651 \u22121\n10 = 9\n10. Hence, for such a constant c, whenever\np \u2265log(cd)\nn\n,\nwith probability at least 9/10 every column has at least one observed entry.\n5. Fix a particular pair of adjacent rows, rows i and i + 1 for some i \u2208{1, . . . , n \u22121}. For\neach column j \u2208{1, . . . , d}, consider the event that both entries Bi,j and Bi+1,j are observed.\nSince each entry is observed independently with probability p, the probability that both are\nobserved in column j is\nP(both Bi,j and Bi+1,j observed) = p2.\nDefine Xi to be the number of columns in which both entries of rows i and i+1 are observed.\nThen Xi is a binomial random variable,\nXi \u223cBinomial(d, p2).\nThe rows i and i + 1 are directly comparable exactly when Xi \u22651, that is, when there is at\nleast one column where both entries are observed. So we want to bound\nP(Xi = 0) = P(rows i and i + 1 are not directly comparable).\nSince Xi \u223cBinomial(d, p2), we have\nP(Xi = 0) = (1 \u2212p2)d.\nAs before, we use the inequality 1 \u2212x \u2264e\u2212x for x \u22650:\n(1 \u2212p2)d \u2264e\u2212p2d.\nNow assume\np \u2265log(cn)\n\u221a\nd\n.\nThen\np2 \u2265(log(cn))2\nd\n,\nand hence\np2d \u2265(log(cn))2.\nTherefore\nP(Xi = 0) \u2264e\u2212p2d \u2264e\u2212(log(cn))2.\n5\n\nSince (log(cn))2 \u2265log(cn) whenever log(cn) \u22651, we have\ne\u2212(log(cn))2 \u2264e\u2212log(cn) = 1\ncn\nfor all sufficiently large n (and fixed c > 1). Thus, for each fixed adjacent pair (i, i + 1),\nP(rows i and i + 1 are not directly comparable) = P(Xi = 0) \u22641\ncn.\nNow we apply a union bound over the n \u22121 adjacent pairs of rows. The event that there\nexists at least one pair (i, i + 1) that is not directly comparable has probability at most\nP(\u2203i \u2208{1, . . . , n\u22121} such that rows i and i+1 are not directly comparable) \u2264\nn\u22121\nX\ni=1\n1\ncn \u2264n \u22121\ncn\n\u22641\nc.\nTherefore\nP(all adjacent pairs of rows are directly comparable) \u22651 \u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22659/10. Hence, for such a constant c, whenever\np \u2265log(cn)\n\u221a\nd\n,\nwith probability at least 9/10 every pair of adjacent rows (i, i + 1) is directly comparable.\n2. SVD Practice (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Let v be a unit norm eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that \u03bb \u22650 and\nthat \u2225Av\u22252 =\n\u221a\n\u03bb.\n2. (2 points) Let v1, v2 be two eigenvectors of A\u22a4A that are orthogonal to each other. Prove\nthat Av1 and Av2 are also orthogonal to each other.\n3. (2 points) Let V \u2208Rd\u00d7d contain the d eigenvectors of A\u22a4A as its columns and let \u039b \u2208Rd\u00d7d\ncontain their corresponding eigenvalues. Assume A\u22a4A is full rank and so all of its eigenvalues\nare positive. Prove that U = AV \u039b\u22121/2 has orthonormal columns. Hint: Apply parts (1) and\n(2).\n4. (2 points) Prove that if V \u2208Rd\u00d7d has orthonormal columns then V V \u22a4= I. Conclude using\npart (3) that we can write\nA = U\u039b1/2V \u22a4,\nwhere U and V have orthonormal columns and \u039b is diagonal. Hint: Use the interpretation\nof V V T as a projection matrix in your proof.\n5. (2 points) Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that Av is an eigenvector\nof AA\u22a4with eigenvalue \u03bb. Conclude that the columns of U from part (4) are eigenvectors of\nAA\u22a4.\n6\n\nSolution:\n1. Let v be a unit eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv\nand\n\u2225v\u22252 = 1.\nMultiply both sides on the left by v\u22a4:\nv\u22a4A\u22a4Av = \u03bb v\u22a4v = \u03bb.\nWe also see that\nv\u22a4A\u22a4Av = (Av)\u22a4(Av) = \u2225Av\u22252\n2.\nPutting these together,\n\u2225Av\u22252\n2 = \u03bb.\nA squared norm is always nonnegative, so \u03bb = \u2225Av\u22252\n2 \u22650. Taking square roots gives\n\u2225Av\u22252 =\n\u221a\n\u03bb.\n2. Suppose v1 and v2 are eigenvectors of A\u22a4A with\nA\u22a4Av1 = \u03bb1v1,\nA\u22a4Av2 = \u03bb2v2,\nand assume v1 and v2 are orthogonal, so v\u22a4\n1 v2 = 0.\nWe want to show that Av1 and Av2 are also orthogonal. Consider their inner product:\n\u27e8Av1, Av2\u27e9= (Av1)\u22a4(Av2) = v\u22a4\n1 A\u22a4Av2.\nSince v2 is an eigenvector of A\u22a4A with eigenvalue \u03bb2, we have\nA\u22a4Av2 = \u03bb2v2,\nso\nv\u22a4\n1 A\u22a4Av2 = v\u22a4\n1 (\u03bb2v2) = \u03bb2 v\u22a4\n1 v2 = \u03bb2 \u00b7 0 = 0.\nTherefore \u27e8Av1, Av2\u27e9= 0, which shows that Av1 and Av2 are orthogonal.\n3. Let V \u2208Rd\u00d7d have as its columns the eigenvectors of A\u22a4A, and let\nA\u22a4AV = V \u039b\nwhere \u039b is the diagonal matrix of eigenvalues. Because A\u22a4A is symmetric and full rank, its\neigenvalues are positive and its eigenvectors can be chosen to be orthonormal. This means\nthe columns of V form an orthonormal basis of Rd and\nV \u22a4V = I.\nWe are given U = AV \u039b\u22121/2 and we want to show the columns of U are orthonormal. We\nthen compute U\u22a4U as such:\nU\u22a4U = (\u039b\u22121/2)\u22a4V \u22a4A\u22a4AV \u039b\u22121/2.\n7\n\nSince \u039b is diagonal with positive entries, \u039b\u22121/2 is also diagonal and symmetric, so (\u039b\u22121/2)\u22a4=\n\u039b\u22121/2. Using A\u22a4AV = V \u039b, we get\nV \u22a4A\u22a4AV = V \u22a4(V \u039b) = (V \u22a4V )\u039b = I\u039b = \u039b.\nTherefore\nU\u22a4U = \u039b\u22121/2 \u039b \u039b\u22121/2 = I.\nSo U\u22a4U = I, which means the columns of U are orthonormal.\n4. Suppose V \u2208Rd\u00d7d has orthonormal columns. V is an orthogonal matrix and both\nV \u22a4V = I\nand\nV V \u22a4= I.\nFrom part (3), we know that\nU = AV \u039b\u22121/2\n=\u21d2\nAV = U\u039b1/2.\nMultiply both sides on the right by V \u22a4:\nAV V \u22a4= U\u039b1/2V \u22a4.\nUsing V V \u22a4= I, the left-hand side becomes simply A, so\nA = U\u039b1/2V \u22a4,\nwhere U and V both have orthonormal columns and \u039b is diagonal with positive entries. This\nis exactly the singular value decomposition of A, with \u039b1/2 containing the singular values.\n5. Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv.\nApply A on the left:\nAA\u22a4(Av) = A(A\u22a4Av) = A(\u03bbv) = \u03bb(Av).\nThis shows that Av is an eigenvector of AA\u22a4with the same eigenvalue \u03bb, where Av \u0338= 0. In\na full-rank setting (where all eigenvalues of A\u22a4A are positive), part (1) tells us that\n\u2225Av\u22252\n2 = \u03bb > 0,\nso Av is nonzero and is a valid eigenvector.\nNow look at the columns of U from part (4). Part (3) defined\nU = AV \u039b\u22121/2,\nwhere V = [v1 v2 \u00b7 \u00b7 \u00b7 vd] contains the eigenvectors of A\u22a4A and\n\u039b\u22121/2 = diag\n\u0012 1\n\u221a\u03bb1\n, . . . ,\n1\n\u221a\u03bbd\n\u0013\n.\n8\n\nMultiplying AV by this diagonal matrix rescales each column. Therefore the jth column of\nU is\nuj =\n1\np\n\u03bbj\nAvj.\nwhere vj is the corresponding eigenvector of A\u22a4A with eigenvalue \u03bbj. As we just showed,\nAvj is an eigenvector of AA\u22a4with eigenvalue \u03bbj, and scaling an eigenvector by a nonzero\nconstant does not change the eigenvalue or the eigenspace.\nTherefore each uj is also an\neigenvector of AA\u22a4with eigenvalue \u03bbj. This means the columns of U form an orthonormal\nset of eigenvectors of AA\u22a4.\n3. Eigendecomposition and Optimal Low-Rank Approximation (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Prove that\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. (2 points) Let V \u2208Rd\u00d7d have orthonormal columns. Prove that for any Z \u2208Rd\u00d7k with\northonormal columns, V \u22a4Z \u2208Rd\u00d7k also has orthonormal columns. Further prove that any\nZ \u2208Rd\u00d7k with orthonormal columns can be written as Z = V \u22a4U for some U \u2208Rd\u00d7k with\northonormal columns. Hint: Use Problem 2.4.\n3. (2 points) Writing A\u22a4A = V \u039bV \u22a4in its eigendecomposition, use part (2) to prove that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. (2 points) Prove that for Z \u2208Rd\u00d7k with orthonormal columns,\ntr(ZZ\u22a4) =\nd\nX\ni=1\n(ZZ\u22a4)ii = k,\nand 0 \u2264(ZZ\u22a4)i,i \u22641 for all i \u2208[d]. Hint: There are several ways to prove the second claim.\nOne is by using Problem 2.4 again.\n(5) (2 points) Use (3) and (4) to show that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nk\nX\ni=1\n\u03bbi(A\u22a4A).\nConclude that Z optimizing the equation in part (1) has as its columns the k eigenvectors of\nA\u22a4A corresponding to the top eigenvalues \u03bb1(A\u22a4A), ..., \u03bbk(A\u22a4A).\n9\n\nSolution:\n1. We begin by expanding the Frobenius norm:\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr\n\u0010\n(A \u2212AZZ\u22a4)\u22a4(A \u2212AZZ\u22a4)\n\u0011\n.\nDistributing the product gives\ntr(A\u22a4A) \u22122 tr(A\u22a4AZZ\u22a4) + tr(ZZ\u22a4A\u22a4AZZ\u22a4).\nUsing the cyclic property of the trace and Z\u22a4Z = I, the last term simplifies:\ntr(ZZ\u22a4A\u22a4AZZ\u22a4) = tr(Z\u22a4A\u22a4AZ).\nThus the expression becomes\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr(A\u22a4A) \u2212tr(Z\u22a4A\u22a4AZ).\nThe first term does not depend on Z, so minimizing the Frobenius error is equivalent to\nmaximizing\ntr(Z\u22a4A\u22a4AZ).\nwhich proves\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. Let V have orthonormal columns, so V \u22a4V = I. If Z also has orthonormal columns, then\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4V V \u22a4Z.\nBecause V V \u22a4= I for an orthonormal basis of Rd, we get\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4Z = I.\nSo V \u22a4Z also has orthonormal columns.\nTo see why every Z with orthonormal columns can be written as Z = V \u22a4U, start by defining\nU = V Z.\nSince V has orthonormal columns, multiplying by V does not change inner products\nU\u22a4U = Z\u22a4V \u22a4V Z = Z\u22a4Z = I,\nso U also has orthonormal columns. Solving for Z gives\nZ = V \u22a4U.\n10\n\n3. Using A\u22a4A = V \u039bV \u22a4, we substitute:\ntr(Z\u22a4A\u22a4AZ) = tr(Z\u22a4V \u039bV \u22a4Z).\nFrom part (2), whenever Z has orthonormal columns, we can write Z = V \u22a4U with U also\northonormal. Substituting,\ntr(Z\u22a4A\u22a4AZ) = tr(U\u22a4\u039bU).\nBecause the mapping Z 7\u2192U is a bijection (Z \u2190\u2192U = V Z) on matrices with orthonormal\ncolumns, we get\nmax\nZ\u22a4Z=I tr(Z\u22a4A\u22a4AZ) = max\nU\u22a4U=I tr(U\u22a4\u039bU) = max\nZ\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. Since Z has orthonormal columns,\nZ\u22a4Z = I.\nUsing the cyclic property of trace,\ntr(ZZ\u22a4) = tr(Z\u22a4Z) = tr(I) = k.\nSince ZZ\u22a4is a projection matrix onto the column space of Z, each diagonal entry satisfies\n0 \u2264(ZZ\u22a4)i,i \u22641.\nThe lower bound holds because a projection cannot produce negative values on the diagonal.\nFor the upper bound, projecting any vector can only make its length shorter or keep it the\nsame, so the squared length of each coordinate direction cannot grow. This means every\ndiagonal entry of ZZ\u22a4must be at most 1.\n5. From part (3), the expression we want to maximize can be written in the diagonal basis as\ntr(Z\u22a4\u039bZ).\nSince \u039b is diagonal with entries \u03bb1 \u2265\u03bb2 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbd, we can expand its trace:\ntr(Z\u22a4\u039bZ) =\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\nPart (4) tells us two facts about the diagonal entries of ZZ\u22a4:\nd\nX\ni=1\n(ZZ\u22a4)i,i = k,\n0 \u2264(ZZ\u22a4)i,i \u22641\nfor all i.\nThis means that the diagonal of ZZ\u22a4behaves like a collection of d numbers between 0 and\n1 whose total adds up to k.\nLooking at the quantity we are trying to maximize:\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\n11\n\nEach diagonal entry (ZZ\u22a4)i,i multiplies the eigenvalue \u03bbi. Since the \u03bbi\u2019s are sorted from\nlargest to smallest, this weighted sum becomes largest when the k units of weight are placed\non the coordinates with the biggest eigenvalues.\nBecause each (ZZ\u22a4)i,i is at most 1, the best we can do is assign value 1 to the first k diagonal\nentries (corresponding to \u03bb1, . . . , \u03bbk) and assign value 0 to all remaining entries. Any other\nchoice would place some weight on a smaller eigenvalue and reduce the total.\nWith this choice, the trace becomes\nk\nX\ni=1\n\u03bbi.\nSo,\nmax\nZ\u22a4Z=I tr(Z\u22a4\u039bZ) =\nk\nX\ni=1\n\u03bbi.\nC2. More Matrix Completion (10 points)\nConsider an n\u00d7d matrix B where each entry is independently observed with probability p. Assume\nd \u2264n.\n1. (2 points) Prove that if:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 No subset of rows is isolated.\nthen B can be reconstructed from the observed entries.\n2. (2 points) Prove that a subset of rows U is isolated with probability at most\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n3. (2 points) Prove that\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U|/2)\nassuming 1 \u2264|U| \u2264n/2 and p \u2265c ln(n)/d for sufficiently large c. Hint: First show\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U| + e\u2212p(n\u22122|U|))\n4. (2 points) Prove that the probability there exists an isolated subset of nodes is at most 1/n\nif p \u2265c ln(n)/d for sufficiently large c. Hint: Use the union bound.\n5. (2 points) Describe an n\u00d7n matrix C with rank 1 such that the probability we can reconstruct\nC is at most 1/2n even if each entry is independently observed with probability 1/2 and\nobservations are independent.\nHint: C has zero entries otherwise p = c(logn)/n would\nsuffice for reconstruction with high probability.\n12\n\nSolution:\n1. Since B has rank 1, there exist vectors u \u2208Rn and v \u2208Rd such that\nB = uv\u22a4\nso\nBij = uivj.\nTo better visualize how the rows relate to each other, we can form a graph whose nodes are\nthe rows of B. Two rows i and j are connected by an edge if they are directly comparable,\nmeaning they share an observed entry in at least one column. If some subset of rows were cut\noff from the rest of the graph, that subset would be isolated, which is ruled out by assumption.\nTherefore this row-graph is connected.\nIf (i, j) is an edge, then there exists a column k with both Bik and Bjk observed. Using\nBik = uivk and Bjk = ujvk and the fact that all observed entries are nonzero, we get\nui\nuj\n= Bik\nBjk\n.\nThus each edge gives a ratio between two entries of u.\nBecause the row-graph is connected, for any row r there is a path\n1 = r0, r1, . . . , rt = r.\nMultiplying ratios along this path gives\nu1\nur\n=\nt\u22121\nY\ns=0\nurs\nurs+1\n=\nt\u22121\nY\ns=0\nBrs,ks\nBrs+1,ks\n.\nChoosing u1 = 1 determines every ur.\nEach column j has at least one observed entry Bi(j),j. Using Bi(j),j = ui(j)vj, we then get\nvj = Bi(j),j\nui(j)\n.\nAll ui and vj are now known, so every entry of B follows from\nBij = uivj.\nThus B can be reconstructed from the observed entries.\n2. Fix a subset of rows U \u2286[n]. In one column j, U is \u201cdisconnected\u201d from its complement Uc\nif either:\n\u2022 no entry in U is observed in column j, or\n\u2022 no entry in Uc is observed in column j.\nThe probability of the first event is (1 \u2212p)|U|, and of the second is (1 \u2212p)n\u2212|U|. By the union\nbound,\nPr(column j does not connect U and Uc) \u2264(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|.\nColumns are independent, so for all d columns,\nPr(U is isolated) \u2264\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n13\n\n3. Let s = |U| with 1 \u2264s \u2264n/2. Set\nx = (1 \u2212p)s,\ny = (1 \u2212p) n\u2212s = (1 \u2212p) s+(n\u22122s) = x \u00b7 (1 \u2212p) n\u22122s.\nThen\nx + y = x\n\u00001 + (1 \u2212p) n\u22122s\u0001\n.\nUsing the inequality (1 \u2212p)k \u2264e\u2212pk for any k \u22650,\nx = (1 \u2212p)s \u2264e\u2212ps,\n(1 \u2212p) n\u22122s \u2264e\u2212p(n\u22122s),\nso\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264e\u2212ps\u00001 + e\u2212p(n\u22122s)\u0001\n.\nFor any z \u22650, 1 + z \u2264ez, giving\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264exp\n\u0000\u2212ps + e\u2212p(n\u22122s)\u0001\n.\nNow assume p \u2265c ln(n)/d with c large. If 1 \u2264s \u2264n/2 then n \u22122s \u22650, so e\u2212p(n\u22122s) is\nvery small. For large enough c, the quantity e\u2212p(n\u22122s) becomes smaller than ps\n2 for all such\ns. Substituting this gives\n\u2212ps + e\u2212p(n\u22122s) \u2264\u2212ps + ps\n2 = \u2212ps\n2 ,\nand therefore\n(1 \u2212p)|U| + (1 \u2212p) n\u2212|U| \u2264exp(\u2212p|U|/2).\n4. We want to bound the probability that there exists at least one isolated subset of rows. By\nsymmetry between a subset U and its complement U c, it is enough to consider subsets with\n1 \u2264|U| \u2264\u230an/2\u230b.\nFrom part (3), for any subset U with s = |U| and 1 \u2264s \u2264n/2, the probability that U is\nisolated satisfies\nPr(U is isolated) \u2264\nh\n(1 \u2212p)s + (1 \u2212p)n\u2212sid\n\u2264exp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing the union bound over all nonempty U with |U| \u2264n/2,\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing\n\u0000n\ns\n\u0001\n\u2264ns and the assumption p \u2265c ln(n)/d, we get\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n\u2264ns exp\n\u0012\n\u2212cs ln n\n2\n\u0013\n= ns\u2212cs\n2 = n\u2212s( c\n2 \u22121).\nIf c is large enough so that c\n2 \u22121 \u22652, then each term is at most n\u22122s, and\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\nn\u22122s \u2264\n\u221e\nX\ns=1\nn\u22122s =\nn\u22122\n1 \u2212n\u22122 \u22641\nn\nfor all n \u22652. Therefore, for p \u2265c ln(n)/d with c sufficiently large, the probability that there\nexists an isolated subset of rows is at most 1/n.\n14\n\n5. Let C be an n \u00d7 n rank-1 matrix with a single nonzero row. For example, take\nu = (1, 0, 0, . . . , 0)\u22a4,\nv \u2208Rn with all entries nonzero,\nand set C = uv\u22a4. Then only the first row of C can be nonzero.\nTo reconstruct C uniquely, we need to know every entry of v, since C1j = vj and all other\nrows are zero. If even one entry in the first row is unobserved, we can change that single\nunobserved value while keeping all observed entries and zeros the same, and obtain a different\nrank-1 matrix consistent with the observations. So reconstruction is only possible if all n\nentries in the first row are observed.\nEach entry is observed independently with probability 1/2, so the probability all n entries in\nthe first row are observed is\n(1/2)n.\nTherefore, the probability we can reconstruct C is at most 1/2n.\n15\n",
        "added_at": "2025-11-28T23:31:46.617572"
      },
      {
        "name": "COMPSCI_514_HW_4.pdf",
        "type": "document",
        "text": "COMPSCI 514 HW 4\nHelektra Katsoulakis\nNovember 2025\nCore Competency Problems\n1. Rank One Matrices and Matrix Completion (10 points)\n1. (2 points) Suppose A \u2208Rn\u00d7d is a rank-1 matrix, i.e., all rows are multiples of some row\nvector y\u22a4. Without loss of generality, you may assume that y is a unit vector. Prove, without\nappealing to the existence of the SVD, that A can be written as \u03b1xy\u22a4for some unit vector\nx \u2208Rn. Specifically, express \u03b1 and each entry of x in terms of entries of A and y.\n2. (2 points) If A = \u03b1xy\u22a4, find the eigenvector of A\u22a4A with the largest eigenvalue and derive an\nexpression for this largest eigenvalue. To get full marks you need to fully explain your work.\n3. (2 points) Let B \u2208Rn\u00d7d be a partially observed matrix, i.e., we know the values of some of\nthe entries but not others. We say rows i, j \u2208[n] of B are directly comparable if there exists\nk such that Bi,k and Bj,k are both known. Prove that the unobserved entries of B can be\ndeduced from the observed entries of B given the following four assumptions:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 For all i \u2208{1, 2, . . . , n \u22121}, the ith and (i + 1)st rows are directly comparable.\n4. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cd)\nn\nfor some sufficiently large constant c then with probability at least 9/10, there exists an\nobserved entry in every column.\n5. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cn)\n\u221a\nd\nfor some sufficiently large constant c then with probability at least 9/10, for all i \u2208{1, 2, . . . , n\u2212\n1}, the ith and (i + 1)st rows are directly comparable. Note: It is also possible to prove that\np \u2265\nr\nlog(cn)\nd\nsuffices. This is only stronger than the bound we are asking for so will receive full credit.\n1\n\nSolution:\n1. Since every row of A is a scalar multiple of y\u22a4, for each i there exists a number ci such that\nAi,: = ci y\u22a4.\nTaking the inner product of the i-th row with y gives\nAi,:y = (ciy\u22a4)y = ci (y\u22a4y) = ci,\nbecause y is a unit vector. Therefore each coefficient is\nci = Ai,:y =\nd\nX\nj=1\nAijyj,\nand the vector z = (c1, . . . , cn)\u22a4is exactly Ay. This shows that\nA = zy\u22a4= (Ay) y\u22a4.\nSince A is rank-1 and nonzero, the vector Ay is not the zero vector. Let\n\u03b1 = \u2225Ay\u22252 =\nv\nu\nu\nu\nt\nn\nX\ni=1\n\uf8eb\n\uf8ed\nd\nX\nj=1\nAijyj\n\uf8f6\n\uf8f8\n2\n,\nx =\nAy\n\u2225Ay\u22252\n.\nThen x is a unit vector by construction, and substituting into the expression for A gives\nA = (Ay) y\u22a4= \u03b1xy\u22a4.\nFinally, each entry of x is\nxi = (Ay)i\n\u2225Ay\u22252\n=\nd\nX\nj=1\nAijyj\nv\nu\nu\nt\nn\nX\nk=1\n d\nX\n\u2113=1\nAk\u2113y\u2113\n!2 .\nWith these definitions of \u03b1 and x, we have shown that\nA = \u03b1xy\u22a4\nfor some unit vector x \u2208Rn.\n2. First compute\nA\u22a4= (\u03b1xy\u22a4)\u22a4= \u03b1yx\u22a4.\nThen\nA\u22a4A = (\u03b1yx\u22a4)(\u03b1xy\u22a4) = \u03b12 y(x\u22a4x) y\u22a4.\n2\n\nSince x is a unit vector, x\u22a4x = \u2225x\u22252\n2 = 1, so this simplifies to\nA\u22a4A = \u03b12 yy\u22a4.\nThis is a rank-1 matrix in Rd\u00d7d with range equal to the span of y.\nNow check what A\u22a4A does to the vector y:\nA\u22a4A y = \u03b12 yy\u22a4y = \u03b12 (y\u22a4y) y = \u03b12 \u00b7 1 \u00b7 y = \u03b12y,\nusing again that y\u22a4y = 1. This shows that y is an eigenvector of A\u22a4A with eigenvalue \u03b12.\nNext, take any vector z \u2208Rd that is orthogonal to y, so y\u22a4z = 0. Then\nA\u22a4A z = \u03b12 yy\u22a4z = \u03b12 y \u00b7 0 = 0.\nThus every vector orthogonal to y is an eigenvector with eigenvalue 0. Therefore the eigen-\nvalues of A\u22a4A are\n\u03bbmax = \u03b12,\nand all remaining eigenvalues are 0.\nThe eigenvector corresponding to the largest eigenvalue \u03b12 is any nonzero multiple of y, and\nif we want a unit eigenvector, we can simply take y itself.\n3. Since B has rank 1, it can be written as\nB = uv\u22a4,\nso each entry factors as\nBij = uivj.\nThis means every row is a scalar multiple of the same vector v, and every column is a scalar\nmultiple of the same vector u.\nFirst, notice that under the assumptions, every row and every column must contain at least\none observed (and nonzero) entry. Because all observed entries satisfy Bij = uivj \u0338= 0, we\nimmediately get that ui \u0338= 0 and vj \u0338= 0 whenever Bij is observed. Since each row participates\nin at least one directly comparable pair, every row has at least one known, nonzero entry, and\ntherefore every ui is nonzero. Similarly, since every column has at least one observed entry,\nevery vj is nonzero as well.\nNow consider two directly comparable rows i and j. By definition, there exists a column k\nsuch that both Bik and Bjk are observed. Using the rank-1 structure,\nBik = uivk,\nBjk = ujvk.\nSince vk \u0338= 0, we divide and get\nBik\nBjk\n= uivk\nujvk\n= ui\nuj\n.\nThus one shared observed entry gives us the ratio ui/uj.\n3\n\nBecause every adjacent pair (i, i + 1) is directly comparable, we can recover all ratios\nu2\nu1\n,\nu3\nu2\n,\n. . . ,\nun\nun\u22121\n.\nMultiplying these together lets us express each ui in terms of u1:\nui\nu1\n=\ni\u22121\nY\nt=1\nut+1\nut\n.\nChoosing any nonzero value for u1 determines every ui uniquely up to an overall scale factor.\nNext, since every column j has at least one observed entry, pick any row i such that Bij is\nknown. Using Bij = uivj and the fact that ui is now known, we can solve for\nvj = Bij\nui\n.\nThis determines every vj.\nFinally, once u and v are known, all entries of B including are determined by\nBij = uivj.\nAfter solving for all of the ui and vj, every entry of B is shown to be the product uivj. Any\ndifferent choice for a missing entry would change this product and would no longer match the\nobserved values, so the missing entries are completely fixed.\n4. Fix a particular column j \u2208{1, . . . , d}.\nThere are n entries in this column, and each is\nobserved independently with probability p. The event that column j has no observed entries\nmeans that all n entries in that column are unobserved, which has probability\nP(column j has no observed entries) = (1 \u2212p)n.\nWe now bound this probability using the inequality 1\u2212p \u2264e\u2212p valid for all real p. This gives\n(1 \u2212p)n \u2264e\u2212pn.\nIf p \u2265log(cd)\nn\n, then\npn \u2265log(cd),\nso\n(1 \u2212p)n \u2264e\u2212pn \u2264e\u2212log(cd) = 1\ncd.\nThus for each fixed column j,\nP(column j has no observed entries) \u22641\ncd.\nWe now apply a union bound over all d columns. The event that at least one column has no\nobserved entries is the union of the events that column j has no observed entries, and so\nP(\u2203a column with no observed entries) \u2264\nd\nX\nj=1\nP(column j has no observed entries) \u2264d\u00b7 1\ncd = 1\nc.\n4\n\nTherefore\nP(every column has at least one observed entry) = 1\u2212P(\u2203a column with no observed entries) \u22651\u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22651 \u22121\n10 = 9\n10. Hence, for such a constant c, whenever\np \u2265log(cd)\nn\n,\nwith probability at least 9/10 every column has at least one observed entry.\n5. Fix a particular pair of adjacent rows, rows i and i + 1 for some i \u2208{1, . . . , n \u22121}. For\neach column j \u2208{1, . . . , d}, consider the event that both entries Bi,j and Bi+1,j are observed.\nSince each entry is observed independently with probability p, the probability that both are\nobserved in column j is\nP(both Bi,j and Bi+1,j observed) = p2.\nDefine Xi to be the number of columns in which both entries of rows i and i+1 are observed.\nThen Xi is a binomial random variable,\nXi \u223cBinomial(d, p2).\nThe rows i and i + 1 are directly comparable exactly when Xi \u22651, that is, when there is at\nleast one column where both entries are observed. So we want to bound\nP(Xi = 0) = P(rows i and i + 1 are not directly comparable).\nSince Xi \u223cBinomial(d, p2), we have\nP(Xi = 0) = (1 \u2212p2)d.\nAs before, we use the inequality 1 \u2212x \u2264e\u2212x for x \u22650:\n(1 \u2212p2)d \u2264e\u2212p2d.\nNow assume\np \u2265log(cn)\n\u221a\nd\n.\nThen\np2 \u2265(log(cn))2\nd\n,\nand hence\np2d \u2265(log(cn))2.\nTherefore\nP(Xi = 0) \u2264e\u2212p2d \u2264e\u2212(log(cn))2.\n5\n\nSince (log(cn))2 \u2265log(cn) whenever log(cn) \u22651, we have\ne\u2212(log(cn))2 \u2264e\u2212log(cn) = 1\ncn\nfor all sufficiently large n (and fixed c > 1). Thus, for each fixed adjacent pair (i, i + 1),\nP(rows i and i + 1 are not directly comparable) = P(Xi = 0) \u22641\ncn.\nNow we apply a union bound over the n \u22121 adjacent pairs of rows. The event that there\nexists at least one pair (i, i + 1) that is not directly comparable has probability at most\nP(\u2203i \u2208{1, . . . , n\u22121} such that rows i and i+1 are not directly comparable) \u2264\nn\u22121\nX\ni=1\n1\ncn \u2264n \u22121\ncn\n\u22641\nc.\nTherefore\nP(all adjacent pairs of rows are directly comparable) \u22651 \u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22659/10. Hence, for such a constant c, whenever\np \u2265log(cn)\n\u221a\nd\n,\nwith probability at least 9/10 every pair of adjacent rows (i, i + 1) is directly comparable.\n2. SVD Practice (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Let v be a unit norm eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that \u03bb \u22650 and\nthat \u2225Av\u22252 =\n\u221a\n\u03bb.\n2. (2 points) Let v1, v2 be two eigenvectors of A\u22a4A that are orthogonal to each other. Prove\nthat Av1 and Av2 are also orthogonal to each other.\n3. (2 points) Let V \u2208Rd\u00d7d contain the d eigenvectors of A\u22a4A as its columns and let \u039b \u2208Rd\u00d7d\ncontain their corresponding eigenvalues. Assume A\u22a4A is full rank and so all of its eigenvalues\nare positive. Prove that U = AV \u039b\u22121/2 has orthonormal columns. Hint: Apply parts (1) and\n(2).\n4. (2 points) Prove that if V \u2208Rd\u00d7d has orthonormal columns then V V \u22a4= I. Conclude using\npart (3) that we can write\nA = U\u039b1/2V \u22a4,\nwhere U and V have orthonormal columns and \u039b is diagonal. Hint: Use the interpretation\nof V V T as a projection matrix in your proof.\n5. (2 points) Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that Av is an eigenvector\nof AA\u22a4with eigenvalue \u03bb. Conclude that the columns of U from part (4) are eigenvectors of\nAA\u22a4.\n6\n\nSolution:\n1. Let v be a unit eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv\nand\n\u2225v\u22252 = 1.\nMultiply both sides on the left by v\u22a4:\nv\u22a4A\u22a4Av = \u03bb v\u22a4v = \u03bb.\nWe also see that\nv\u22a4A\u22a4Av = (Av)\u22a4(Av) = \u2225Av\u22252\n2.\nPutting these together,\n\u2225Av\u22252\n2 = \u03bb.\nA squared norm is always nonnegative, so \u03bb = \u2225Av\u22252\n2 \u22650. Taking square roots gives\n\u2225Av\u22252 =\n\u221a\n\u03bb.\n2. Suppose v1 and v2 are eigenvectors of A\u22a4A with\nA\u22a4Av1 = \u03bb1v1,\nA\u22a4Av2 = \u03bb2v2,\nand assume v1 and v2 are orthogonal, so v\u22a4\n1 v2 = 0.\nWe want to show that Av1 and Av2 are also orthogonal. Consider their inner product:\n\u27e8Av1, Av2\u27e9= (Av1)\u22a4(Av2) = v\u22a4\n1 A\u22a4Av2.\nSince v2 is an eigenvector of A\u22a4A with eigenvalue \u03bb2, we have\nA\u22a4Av2 = \u03bb2v2,\nso\nv\u22a4\n1 A\u22a4Av2 = v\u22a4\n1 (\u03bb2v2) = \u03bb2 v\u22a4\n1 v2 = \u03bb2 \u00b7 0 = 0.\nTherefore \u27e8Av1, Av2\u27e9= 0, which shows that Av1 and Av2 are orthogonal.\n3. Let V \u2208Rd\u00d7d have as its columns the eigenvectors of A\u22a4A, and let\nA\u22a4AV = V \u039b\nwhere \u039b is the diagonal matrix of eigenvalues. Because A\u22a4A is symmetric and full rank, its\neigenvalues are positive and its eigenvectors can be chosen to be orthonormal. This means\nthe columns of V form an orthonormal basis of Rd and\nV \u22a4V = I.\nWe are given U = AV \u039b\u22121/2 and we want to show the columns of U are orthonormal. We\nthen compute U\u22a4U as such:\nU\u22a4U = (\u039b\u22121/2)\u22a4V \u22a4A\u22a4AV \u039b\u22121/2.\n7\n\nSince \u039b is diagonal with positive entries, \u039b\u22121/2 is also diagonal and symmetric, so (\u039b\u22121/2)\u22a4=\n\u039b\u22121/2. Using A\u22a4AV = V \u039b, we get\nV \u22a4A\u22a4AV = V \u22a4(V \u039b) = (V \u22a4V )\u039b = I\u039b = \u039b.\nTherefore\nU\u22a4U = \u039b\u22121/2 \u039b \u039b\u22121/2 = I.\nSo U\u22a4U = I, which means the columns of U are orthonormal.\n4. Suppose V \u2208Rd\u00d7d has orthonormal columns. V is an orthogonal matrix and both\nV \u22a4V = I\nand\nV V \u22a4= I.\nFrom part (3), we know that\nU = AV \u039b\u22121/2\n=\u21d2\nAV = U\u039b1/2.\nMultiply both sides on the right by V \u22a4:\nAV V \u22a4= U\u039b1/2V \u22a4.\nUsing V V \u22a4= I, the left-hand side becomes simply A, so\nA = U\u039b1/2V \u22a4,\nwhere U and V both have orthonormal columns and \u039b is diagonal with positive entries. This\nis exactly the singular value decomposition of A, with \u039b1/2 containing the singular values.\n5. Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv.\nApply A on the left:\nAA\u22a4(Av) = A(A\u22a4Av) = A(\u03bbv) = \u03bb(Av).\nThis shows that Av is an eigenvector of AA\u22a4with the same eigenvalue \u03bb, where Av \u0338= 0. In\na full-rank setting (where all eigenvalues of A\u22a4A are positive), part (1) tells us that\n\u2225Av\u22252\n2 = \u03bb > 0,\nso Av is nonzero and is a valid eigenvector.\nNow look at the columns of U from part (4). Part (3) defined\nU = AV \u039b\u22121/2,\nwhere V = [v1 v2 \u00b7 \u00b7 \u00b7 vd] contains the eigenvectors of A\u22a4A and\n\u039b\u22121/2 = diag\n\u0012 1\n\u221a\u03bb1\n, . . . ,\n1\n\u221a\u03bbd\n\u0013\n.\n8\n\nMultiplying AV by this diagonal matrix rescales each column. Therefore the jth column of\nU is\nuj =\n1\np\n\u03bbj\nAvj.\nwhere vj is the corresponding eigenvector of A\u22a4A with eigenvalue \u03bbj. As we just showed,\nAvj is an eigenvector of AA\u22a4with eigenvalue \u03bbj, and scaling an eigenvector by a nonzero\nconstant does not change the eigenvalue or the eigenspace.\nTherefore each uj is also an\neigenvector of AA\u22a4with eigenvalue \u03bbj. This means the columns of U form an orthonormal\nset of eigenvectors of AA\u22a4.\n3. Eigendecomposition and Optimal Low-Rank Approximation (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Prove that\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. (2 points) Let V \u2208Rd\u00d7d have orthonormal columns. Prove that for any Z \u2208Rd\u00d7k with\northonormal columns, V \u22a4Z \u2208Rd\u00d7k also has orthonormal columns. Further prove that any\nZ \u2208Rd\u00d7k with orthonormal columns can be written as Z = V \u22a4U for some U \u2208Rd\u00d7k with\northonormal columns. Hint: Use Problem 2.4.\n3. (2 points) Writing A\u22a4A = V \u039bV \u22a4in its eigendecomposition, use part (2) to prove that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. (2 points) Prove that for Z \u2208Rd\u00d7k with orthonormal columns,\ntr(ZZ\u22a4) =\nd\nX\ni=1\n(ZZ\u22a4)ii = k,\nand 0 \u2264(ZZ\u22a4)i,i \u22641 for all i \u2208[d]. Hint: There are several ways to prove the second claim.\nOne is by using Problem 2.4 again.\n(5) (2 points) Use (3) and (4) to show that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nk\nX\ni=1\n\u03bbi(A\u22a4A).\nConclude that Z optimizing the equation in part (1) has as its columns the k eigenvectors of\nA\u22a4A corresponding to the top eigenvalues \u03bb1(A\u22a4A), ..., \u03bbk(A\u22a4A).\n9\n\nSolution:\n1. We begin by expanding the Frobenius norm:\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr\n\u0010\n(A \u2212AZZ\u22a4)\u22a4(A \u2212AZZ\u22a4)\n\u0011\n.\nDistributing the product gives\ntr(A\u22a4A) \u22122 tr(A\u22a4AZZ\u22a4) + tr(ZZ\u22a4A\u22a4AZZ\u22a4).\nUsing the cyclic property of the trace and Z\u22a4Z = I, the last term simplifies:\ntr(ZZ\u22a4A\u22a4AZZ\u22a4) = tr(Z\u22a4A\u22a4AZ).\nThus the expression becomes\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr(A\u22a4A) \u2212tr(Z\u22a4A\u22a4AZ).\nThe first term does not depend on Z, so minimizing the Frobenius error is equivalent to\nmaximizing\ntr(Z\u22a4A\u22a4AZ).\nwhich proves\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. Let V have orthonormal columns, so V \u22a4V = I. If Z also has orthonormal columns, then\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4V V \u22a4Z.\nBecause V V \u22a4= I for an orthonormal basis of Rd, we get\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4Z = I.\nSo V \u22a4Z also has orthonormal columns.\nTo see why every Z with orthonormal columns can be written as Z = V \u22a4U, start by defining\nU = V Z.\nSince V has orthonormal columns, multiplying by V does not change inner products\nU\u22a4U = Z\u22a4V \u22a4V Z = Z\u22a4Z = I,\nso U also has orthonormal columns. Solving for Z gives\nZ = V \u22a4U.\n10\n\n3. Using A\u22a4A = V \u039bV \u22a4, we substitute:\ntr(Z\u22a4A\u22a4AZ) = tr(Z\u22a4V \u039bV \u22a4Z).\nFrom part (2), whenever Z has orthonormal columns, we can write Z = V \u22a4U with U also\northonormal. Substituting,\ntr(Z\u22a4A\u22a4AZ) = tr(U\u22a4\u039bU).\nBecause the mapping Z 7\u2192U is a bijection (Z \u2190\u2192U = V Z) on matrices with orthonormal\ncolumns, we get\nmax\nZ\u22a4Z=I tr(Z\u22a4A\u22a4AZ) = max\nU\u22a4U=I tr(U\u22a4\u039bU) = max\nZ\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. Since Z has orthonormal columns,\nZ\u22a4Z = I.\nUsing the cyclic property of trace,\ntr(ZZ\u22a4) = tr(Z\u22a4Z) = tr(I) = k.\nSince ZZ\u22a4is a projection matrix onto the column space of Z, each diagonal entry satisfies\n0 \u2264(ZZ\u22a4)i,i \u22641.\nThe lower bound holds because a projection cannot produce negative values on the diagonal.\nFor the upper bound, projecting any vector can only make its length shorter or keep it the\nsame, so the squared length of each coordinate direction cannot grow. This means every\ndiagonal entry of ZZ\u22a4must be at most 1.\n5. From part (3), the expression we want to maximize can be written in the diagonal basis as\ntr(Z\u22a4\u039bZ).\nSince \u039b is diagonal with entries \u03bb1 \u2265\u03bb2 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbd, we can expand its trace:\ntr(Z\u22a4\u039bZ) =\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\nPart (4) tells us two facts about the diagonal entries of ZZ\u22a4:\nd\nX\ni=1\n(ZZ\u22a4)i,i = k,\n0 \u2264(ZZ\u22a4)i,i \u22641\nfor all i.\nThis means that the diagonal of ZZ\u22a4behaves like a collection of d numbers between 0 and\n1 whose total adds up to k.\nLooking at the quantity we are trying to maximize:\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\n11\n\nEach diagonal entry (ZZ\u22a4)i,i multiplies the eigenvalue \u03bbi. Since the \u03bbi\u2019s are sorted from\nlargest to smallest, this weighted sum becomes largest when the k units of weight are placed\non the coordinates with the biggest eigenvalues.\nBecause each (ZZ\u22a4)i,i is at most 1, the best we can do is assign value 1 to the first k diagonal\nentries (corresponding to \u03bb1, . . . , \u03bbk) and assign value 0 to all remaining entries. Any other\nchoice would place some weight on a smaller eigenvalue and reduce the total.\nWith this choice, the trace becomes\nk\nX\ni=1\n\u03bbi.\nSo,\nmax\nZ\u22a4Z=I tr(Z\u22a4\u039bZ) =\nk\nX\ni=1\n\u03bbi.\nC2. More Matrix Completion (10 points)\nConsider an n\u00d7d matrix B where each entry is independently observed with probability p. Assume\nd \u2264n.\n1. (2 points) Prove that if:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 No subset of rows is isolated.\nthen B can be reconstructed from the observed entries.\n2. (2 points) Prove that a subset of rows U is isolated with probability at most\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n3. (2 points) Prove that\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U|/2)\nassuming 1 \u2264|U| \u2264n/2 and p \u2265c ln(n)/d for sufficiently large c. Hint: First show\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U| + e\u2212p(n\u22122|U|))\n4. (2 points) Prove that the probability there exists an isolated subset of nodes is at most 1/n\nif p \u2265c ln(n)/d for sufficiently large c. Hint: Use the union bound.\n5. (2 points) Describe an n\u00d7n matrix C with rank 1 such that the probability we can reconstruct\nC is at most 1/2n even if each entry is independently observed with probability 1/2 and\nobservations are independent.\nHint: C has zero entries otherwise p = c(logn)/n would\nsuffice for reconstruction with high probability.\n12\n\nSolution:\n1. Since B has rank 1, there exist vectors u \u2208Rn and v \u2208Rd such that\nB = uv\u22a4\nso\nBij = uivj.\nTo better visualize how the rows relate to each other, we can form a graph whose nodes are\nthe rows of B. Two rows i and j are connected by an edge if they are directly comparable,\nmeaning they share an observed entry in at least one column. If some subset of rows were cut\noff from the rest of the graph, that subset would be isolated, which is ruled out by assumption.\nTherefore this row-graph is connected.\nIf (i, j) is an edge, then there exists a column k with both Bik and Bjk observed. Using\nBik = uivk and Bjk = ujvk and the fact that all observed entries are nonzero, we get\nui\nuj\n= Bik\nBjk\n.\nThus each edge gives a ratio between two entries of u.\nBecause the row-graph is connected, for any row r there is a path\n1 = r0, r1, . . . , rt = r.\nMultiplying ratios along this path gives\nu1\nur\n=\nt\u22121\nY\ns=0\nurs\nurs+1\n=\nt\u22121\nY\ns=0\nBrs,ks\nBrs+1,ks\n.\nChoosing u1 = 1 determines every ur.\nEach column j has at least one observed entry Bi(j),j. Using Bi(j),j = ui(j)vj, we then get\nvj = Bi(j),j\nui(j)\n.\nAll ui and vj are now known, so every entry of B follows from\nBij = uivj.\nThus B can be reconstructed from the observed entries.\n2. Fix a subset of rows U \u2286[n]. In one column j, U is \u201cdisconnected\u201d from its complement Uc\nif either:\n\u2022 no entry in U is observed in column j, or\n\u2022 no entry in Uc is observed in column j.\nThe probability of the first event is (1 \u2212p)|U|, and of the second is (1 \u2212p)n\u2212|U|. By the union\nbound,\nPr(column j does not connect U and Uc) \u2264(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|.\nColumns are independent, so for all d columns,\nPr(U is isolated) \u2264\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n13\n\n3. Let s = |U| with 1 \u2264s \u2264n/2. Set\nx = (1 \u2212p)s,\ny = (1 \u2212p) n\u2212s = (1 \u2212p) s+(n\u22122s) = x \u00b7 (1 \u2212p) n\u22122s.\nThen\nx + y = x\n\u00001 + (1 \u2212p) n\u22122s\u0001\n.\nUsing the inequality (1 \u2212p)k \u2264e\u2212pk for any k \u22650,\nx = (1 \u2212p)s \u2264e\u2212ps,\n(1 \u2212p) n\u22122s \u2264e\u2212p(n\u22122s),\nso\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264e\u2212ps\u00001 + e\u2212p(n\u22122s)\u0001\n.\nFor any z \u22650, 1 + z \u2264ez, giving\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264exp\n\u0000\u2212ps + e\u2212p(n\u22122s)\u0001\n.\nNow assume p \u2265c ln(n)/d with c large. If 1 \u2264s \u2264n/2 then n \u22122s \u22650, so e\u2212p(n\u22122s) is\nvery small. For large enough c, the quantity e\u2212p(n\u22122s) becomes smaller than ps\n2 for all such\ns. Substituting this gives\n\u2212ps + e\u2212p(n\u22122s) \u2264\u2212ps + ps\n2 = \u2212ps\n2 ,\nand therefore\n(1 \u2212p)|U| + (1 \u2212p) n\u2212|U| \u2264exp(\u2212p|U|/2).\n4. We want to bound the probability that there exists at least one isolated subset of rows. By\nsymmetry between a subset U and its complement U c, it is enough to consider subsets with\n1 \u2264|U| \u2264\u230an/2\u230b.\nFrom part (3), for any subset U with s = |U| and 1 \u2264s \u2264n/2, the probability that U is\nisolated satisfies\nPr(U is isolated) \u2264\nh\n(1 \u2212p)s + (1 \u2212p)n\u2212sid\n\u2264exp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing the union bound over all nonempty U with |U| \u2264n/2,\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing\n\u0000n\ns\n\u0001\n\u2264ns and the assumption p \u2265c ln(n)/d, we get\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n\u2264ns exp\n\u0012\n\u2212cs ln n\n2\n\u0013\n= ns\u2212cs\n2 = n\u2212s( c\n2 \u22121).\nIf c is large enough so that c\n2 \u22121 \u22652, then each term is at most n\u22122s, and\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\nn\u22122s \u2264\n\u221e\nX\ns=1\nn\u22122s =\nn\u22122\n1 \u2212n\u22122 \u22641\nn\nfor all n \u22652. Therefore, for p \u2265c ln(n)/d with c sufficiently large, the probability that there\nexists an isolated subset of rows is at most 1/n.\n14\n\n5. Let C be an n \u00d7 n rank-1 matrix with a single nonzero row. For example, take\nu = (1, 0, 0, . . . , 0)\u22a4,\nv \u2208Rn with all entries nonzero,\nand set C = uv\u22a4. Then only the first row of C can be nonzero.\nTo reconstruct C uniquely, we need to know every entry of v, since C1j = vj and all other\nrows are zero. If even one entry in the first row is unobserved, we can change that single\nunobserved value while keeping all observed entries and zeros the same, and obtain a different\nrank-1 matrix consistent with the observations. So reconstruction is only possible if all n\nentries in the first row are observed.\nEach entry is observed independently with probability 1/2, so the probability all n entries in\nthe first row are observed is\n(1/2)n.\nTherefore, the probability we can reconstruct C is at most 1/2n.\n15\n",
        "added_at": "2025-11-28T23:32:36.192645"
      }
    ]
  }
}